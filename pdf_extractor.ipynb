{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0rnmRZCkKSq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils\n",
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wj0bUPkylsIF",
        "outputId": "2b5e3d2a-ebb6-42bf-a759-3459f5102aa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.8 [186 kB]\n",
            "Fetched 186 kB in 0s (907 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.8_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdf2image pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EZ4kmqNmW8R",
        "outputId": "5c0d4c4f-541b-4877-cc17-b0b2f6625f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import pdf2image\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "from typing import Dict, List, Optional\n",
        "import time\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ClientDataExtractor:\n",
        "    def __init__(self, api_key: str):\n",
        "        \"\"\"\n",
        "        Initialize the Client Data Extractor with GPT-4 Vision\n",
        "\n",
        "        Args:\n",
        "            api_key: OpenAI API key\n",
        "        \"\"\"\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.metrics = [\n",
        "            \"page_number\", \"date\", \"Timestamp\", \"Client_Id\",\n",
        "            \"Company_Name\", \"Address\", \"Contact/Attorneys\",\n",
        "            \"Phone\", \"Fax1\", \"Fax2\", \"Send Fx1\", \"Send Fx2\", \"Status\", \"Email\",\n",
        "            \"Default_Warrant\", \"Manager\", \"Billing_Type\", \"A/R_Balance\"\n",
        "        ]\n",
        "\n",
        "    def pdf_to_images(self, pdf_path: str, max_pages: int = 20) -> List[Image.Image]:\n",
        "        \"\"\"\n",
        "        Convert PDF pages to images with multiple fallback methods\n",
        "\n",
        "        Args:\n",
        "            pdf_path: Path to the PDF file\n",
        "            max_pages: Maximum number of pages to process (to control costs)\n",
        "\n",
        "        Returns:\n",
        "            List of PIL Images\n",
        "        \"\"\"\n",
        "        print(f\"\\n=== Converting PDF to Images: {pdf_path} ===\")\n",
        "        print(f\"File exists: {os.path.exists(pdf_path)}\")\n",
        "        print(f\"File size: {os.path.getsize(pdf_path) / (1024*1024):.2f} MB\")\n",
        "\n",
        "        # Method 1: Standard pdf2image\n",
        "        try:\n",
        "            print(\"Attempting Method 1: Standard pdf2image...\")\n",
        "            images = pdf2image.convert_from_path(\n",
        "                pdf_path,\n",
        "                dpi=300,\n",
        "                first_page=1,\n",
        "                last_page=max_pages,\n",
        "                fmt='PNG'\n",
        "            )\n",
        "\n",
        "            if images:\n",
        "                print(f\"✅ Method 1 Success: Converted {len(images)} pages\")\n",
        "                return images\n",
        "            else:\n",
        "                print(\"❌ Method 1: No images returned\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Method 1 Error: {str(e)}\")\n",
        "\n",
        "        # Method 2: Try with different parameters\n",
        "        try:\n",
        "            print(\"Attempting Method 2: Lower DPI and different settings...\")\n",
        "            images = pdf2image.convert_from_path(\n",
        "                pdf_path,\n",
        "                dpi=200,  # Lower DPI\n",
        "                first_page=1,\n",
        "                last_page=min(5, max_pages),  # Fewer pages initially\n",
        "                fmt='JPEG',\n",
        "                thread_count=1,\n",
        "                use_pdftocairo=False\n",
        "            )\n",
        "\n",
        "            if images:\n",
        "                print(f\"✅ Method 2 Success: Converted {len(images)} pages\")\n",
        "                return images\n",
        "            else:\n",
        "                print(\"❌ Method 2: No images returned\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Method 2 Error: {str(e)}\")\n",
        "\n",
        "        # Method 3: Try with pdftocairo\n",
        "        try:\n",
        "            print(\"Attempting Method 3: Using pdftocairo...\")\n",
        "            images = pdf2image.convert_from_path(\n",
        "                pdf_path,\n",
        "                dpi=150,\n",
        "                first_page=1,\n",
        "                last_page=3,  # Just first 3 pages\n",
        "                use_pdftocairo=True\n",
        "            )\n",
        "\n",
        "            if images:\n",
        "                print(f\"✅ Method 3 Success: Converted {len(images)} pages\")\n",
        "                return images\n",
        "            else:\n",
        "                print(\"❌ Method 3: No images returned\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Method 3 Error: {str(e)}\")\n",
        "\n",
        "        # Method 4: Try with PyMuPDF as fallback\n",
        "        try:\n",
        "            print(\"Attempting Method 4: PyMuPDF fallback...\")\n",
        "            import fitz  # PyMuPDF\n",
        "\n",
        "            doc = fitz.open(pdf_path)\n",
        "            images = []\n",
        "\n",
        "            for page_num in range(min(max_pages, len(doc))):\n",
        "                page = doc.load_page(page_num)\n",
        "                mat = fitz.Matrix(2, 2)  # 2x zoom\n",
        "                pix = page.get_pixmap(matrix=mat)\n",
        "                img_data = pix.tobytes(\"png\")\n",
        "                img = Image.open(BytesIO(img_data))\n",
        "                images.append(img)\n",
        "\n",
        "                if len(images) >= 5:  # Limit for testing\n",
        "                    break\n",
        "\n",
        "            doc.close()\n",
        "\n",
        "            if images:\n",
        "                print(f\"✅ Method 4 Success: Converted {len(images)} pages using PyMuPDF\")\n",
        "                return images\n",
        "            else:\n",
        "                print(\"❌ Method 4: No images returned\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"❌ Method 4: PyMuPDF not installed (pip install PyMuPDF)\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Method 4 Error: {str(e)}\")\n",
        "\n",
        "        print(\"❌ All methods failed to convert PDF to images\")\n",
        "        return []\n",
        "\n",
        "    def encode_image(self, image: Image.Image) -> str:\n",
        "        \"\"\"\n",
        "        Encode PIL Image to base64 string\n",
        "\n",
        "        Args:\n",
        "            image: PIL Image object\n",
        "\n",
        "        Returns:\n",
        "            Base64 encoded string\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Resize image if too large (to control API costs)\n",
        "            max_size = (2048, 2048)\n",
        "            if image.size[0] > max_size[0] or image.size[1] > max_size[1]:\n",
        "                image.thumbnail(max_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "            buffer = BytesIO()\n",
        "            image.save(buffer, format='PNG')\n",
        "            buffer.seek(0)\n",
        "\n",
        "            return base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error encoding image: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def create_vision_extraction_prompt(self, document_name: str = \"\") -> str:\n",
        "        \"\"\"\n",
        "        Create a structured prompt for client data extraction using vision\n",
        "\n",
        "        Args:\n",
        "            document_name: Name of the document (optional)\n",
        "\n",
        "        Returns:\n",
        "            Formatted prompt string\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"\n",
        "You are a professional data analyst analyzing business documents. Extract ALL client and business information entries from the images provided.\n",
        "\n",
        "Document: {document_name if document_name else \"Identify from document\"}\n",
        "\n",
        "Look for client information, contact details, business data, and account information in these images. EXTRACT ALL ENTRIES - do not focus on just one entry.\n",
        "\n",
        "Extract the following data fields for EVERY ENTRY found and return them in JSON format:\n",
        "\n",
        "Required Fields for Each Entry:\n",
        "- page_number: The page number where information is found\n",
        "- date: Any date mentioned in the document (format: YYYY-MM-DD if possible)\n",
        "- Timestamp: Any timestamp or date/time information\n",
        "- Client_Id: Client ID, Account Number, or any unique identifier\n",
        "- Company_Name: Company or business name\n",
        "- Address: Full address (street, city, state, zip/postal code)\n",
        "- Contact/Attorneys: Contact person name or attorney name\n",
        "- Phone: Phone number (include area code)\n",
        "- Fax1: Primary fax number\n",
        "- Fax2: Secondary fax number (if available)\n",
        "- Send Fx1: Send fax for primary fax number\n",
        "- Send Fx2: Send for secondary fax number\n",
        "- Status: Account status, case status, or any status information\n",
        "- Email: Email address\n",
        "- Default_Warrant: Default warrant information or legal notes\n",
        "- Manager: Manager name or account manager\n",
        "- Billing_Type: Billing type, payment method, or billing information\n",
        "- A/R_Balance: Accounts receivable balance or outstanding amount\n",
        "\n",
        "CRITICAL Instructions:\n",
        "1. Extract EVERY SINGLE ENTRY found in the document - do not skip any\n",
        "2. Look for tables with multiple rows - each row is typically a separate entry\n",
        "3. Look for lists, forms with multiple sections, or repeated patterns\n",
        "4. If you find a table with 10 rows of data, extract all 10 entries\n",
        "5. Each entry should be a complete set of the required fields\n",
        "6. For addresses, combine all address components into one field per entry\n",
        "7. For phone/fax numbers, include area codes and formatting as shown\n",
        "8. If a field is not visible for a specific entry, use \"Not Found\" for that field only\n",
        "9. Pay attention to labels, headers, and field names that might indicate the data type\n",
        "10. For monetary amounts (A/R_Balance), include currency symbol if shown\n",
        "11. Number each entry sequentially (entry_1, entry_2, etc.)\n",
        "\n",
        "Return the data in this exact JSON format for MULTIPLE ENTRIES:\n",
        "{{\n",
        "    \"document_info\": {{\n",
        "        \"document_name\": \"{document_name}\",\n",
        "        \"extraction_timestamp\": \"{datetime.now().isoformat()}\",\n",
        "        \"total_pages_processed\": \"number of pages analyzed\",\n",
        "        \"total_entries_found\": \"number of entries extracted\"\n",
        "    }},\n",
        "    \"all_entries\": [\n",
        "        {{\n",
        "            \"entry_id\": \"entry_1\",\n",
        "            \"page_number\": \"value or Not Found\",\n",
        "            \"date\": \"value or Not Found\",\n",
        "            \"Timestamp\": \"value or Not Found\",\n",
        "            \"Client_Id\": \"value or Not Found\",\n",
        "            \"Company_Name\": \"value or Not Found\",\n",
        "            \"Address\": \"value or Not Found\",\n",
        "            \"Contact/Attorneys\": \"value or Not Found\",\n",
        "            \"Phone\": \"value or Not Found\",\n",
        "            \"Fax1\": \"value or Not Found\",\n",
        "            \"Fax2\": \"value or Not Found\",\n",
        "            \"Send Fx1\": \"value or Not Found\",\n",
        "            \"Send Fx2\": \"value or Not Found\",\n",
        "            \"Status\": \"value or Not Found\",\n",
        "            \"Email\": \"value or Not Found\",\n",
        "            \"Default_Warrant\": \"value or Not Found\",\n",
        "            \"Manager\": \"value or Not Found\",\n",
        "            \"Billing_Type\": \"value or Not Found\",\n",
        "            \"A/R_Balance\": \"value or Not Found\"\n",
        "        }},\n",
        "        {{\n",
        "            \"entry_id\": \"entry_2\",\n",
        "            \"page_number\": \"value or Not Found\",\n",
        "            \"date\": \"value or Not Found\",\n",
        "            \"Timestamp\": \"value or Not Found\",\n",
        "            \"Client_Id\": \"value or Not Found\",\n",
        "            \"Company_Name\": \"value or Not Found\",\n",
        "            \"Address\": \"value or Not Found\",\n",
        "            \"Contact/Attorneys\": \"value or Not Found\",\n",
        "            \"Phone\": \"value or Not Found\",\n",
        "            \"Fax1\": \"value or Not Found\",\n",
        "            \"Fax2\": \"value or Not Found\",\n",
        "            \"Send Fx1\": \"value or Not Found\",\n",
        "            \"Send Fx2\": \"value or Not Found\",\n",
        "            \"Status\": \"value or Not Found\",\n",
        "            \"Email\": \"value or Not Found\",\n",
        "            \"Default_Warrant\": \"value or Not Found\",\n",
        "            \"Manager\": \"value or Not Found\",\n",
        "            \"Billing_Type\": \"value or Not Found\",\n",
        "            \"A/R_Balance\": \"value or Not Found\"\n",
        "        }}\n",
        "    ],\n",
        "    \"confidence_score\": \"high/medium/low\",\n",
        "    \"notes\": \"Total entries found and any relevant notes about data extraction\"\n",
        "}}\n",
        "\n",
        "IMPORTANT: If only one entry exists, still use the array format with one object. If no entries are found, return an empty array for \"all_entries\".\n",
        "\n",
        "Analyze all the provided images carefully and extract EVERY client data entry. Look for patterns in forms, tables, or structured layouts that contain this information. Do not limit yourself to just the first or most complete entry.\n",
        "\"\"\"\n",
        "        return prompt\n",
        "\n",
        "    def extract_client_data_from_images(self, images: List[Image.Image], document_name: str = \"\") -> Dict:\n",
        "        \"\"\"\n",
        "        Use GPT-4 Vision to extract client data from PDF images\n",
        "\n",
        "        Args:\n",
        "            images: List of PIL Images from PDF pages\n",
        "            document_name: Name of the document\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing extracted client data\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if not images:\n",
        "                logger.error(\"No images provided for extraction\")\n",
        "                return self._create_empty_response(document_name, \"No images provided\")\n",
        "\n",
        "            print(f\"Processing {len(images)} images for {document_name}\")\n",
        "\n",
        "            # Prepare images for API call\n",
        "            image_messages = []\n",
        "            for i, image in enumerate(images):\n",
        "                print(f\"Encoding image {i+1}/{len(images)} - Size: {image.size}\")\n",
        "                encoded_image = self.encode_image(image)\n",
        "                if encoded_image:\n",
        "                    image_messages.append({\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\n",
        "                            \"url\": f\"data:image/png;base64,{encoded_image}\",\n",
        "                            \"detail\": \"high\"  # Use high detail for better text recognition\n",
        "                        }\n",
        "                    })\n",
        "                    print(f\"Successfully encoded image {i+1}\")\n",
        "                else:\n",
        "                    print(f\"Failed to encode image {i+1}\")\n",
        "\n",
        "                # Limit number of images to control costs\n",
        "                if len(image_messages) >= 10:  # Increased limit for client documents\n",
        "                    logger.info(f\"Limited to first 10 pages for cost control\")\n",
        "                    break\n",
        "\n",
        "            if not image_messages:\n",
        "                logger.error(\"No images could be encoded\")\n",
        "                return self._create_empty_response(document_name, \"No images could be encoded\")\n",
        "\n",
        "            print(f\"Prepared {len(image_messages)} images for API call\")\n",
        "\n",
        "            # Create the prompt\n",
        "            prompt = self.create_vision_extraction_prompt(document_name)\n",
        "\n",
        "            # Prepare messages for API call\n",
        "            messages = [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a professional data analyst with expertise in reading business documents, legal documents, client files, and extracting structured information accurately from various document types including forms, tables, and reports.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": prompt},\n",
        "                        *image_messages\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            print(\"Making API call to GPT-4 Vision...\")\n",
        "\n",
        "            # Make API call to GPT-4 Vision\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o\",  # Use GPT-4 Vision model\n",
        "                messages=messages,\n",
        "                max_tokens=4000,  # Increased token limit for multiple entries\n",
        "                temperature=0  # Low temperature for consistent extraction\n",
        "            )\n",
        "\n",
        "            response_text = response.choices[0].message.content\n",
        "            print(f\"Received response from API (length: {len(response_text)})\")\n",
        "            print(f\"Response preview: {response_text[:500]}...\")\n",
        "\n",
        "            # Try to parse JSON response\n",
        "            try:\n",
        "                # Find JSON in response (in case there's additional text)\n",
        "                start_idx = response_text.find('{')\n",
        "                end_idx = response_text.rfind('}') + 1\n",
        "\n",
        "                if start_idx == -1 or end_idx == 0:\n",
        "                    logger.error(\"No JSON found in response\")\n",
        "                    print(f\"Full response: {response_text}\")\n",
        "                    return self._create_empty_response(document_name, \"No JSON in API response\")\n",
        "\n",
        "                json_str = response_text[start_idx:end_idx]\n",
        "                print(f\"Extracted JSON string: {json_str[:200]}...\")\n",
        "\n",
        "                client_data = json.loads(json_str)\n",
        "                logger.info(f\"Successfully extracted client data for {document_name}\")\n",
        "                return client_data\n",
        "\n",
        "            except json.JSONDecodeError as je:\n",
        "                logger.error(f\"Failed to parse JSON response for {document_name}: {str(je)}\")\n",
        "                print(f\"JSON Error: {str(je)}\")\n",
        "                print(f\"Attempted to parse: {json_str[:500]}...\")\n",
        "                return self._create_empty_response(document_name, f\"JSON parse error: {str(je)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in GPT-4 Vision extraction for {document_name}: {str(e)}\")\n",
        "            print(f\"Full error details: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return self._create_empty_response(document_name, f\"API error: {str(e)}\")\n",
        "\n",
        "    def _create_empty_response(self, document_name: str, error_reason: str = \"Extraction failed\") -> Dict:\n",
        "        \"\"\"Create empty response structure when extraction fails\"\"\"\n",
        "        return {\n",
        "            \"document_info\": {\n",
        "                \"document_name\": document_name,\n",
        "                \"extraction_timestamp\": datetime.now().isoformat(),\n",
        "                \"total_pages_processed\": \"0\",\n",
        "                \"total_entries_found\": \"0\"\n",
        "            },\n",
        "            \"all_entries\": [],\n",
        "            \"confidence_score\": \"low\",\n",
        "            \"notes\": error_reason\n",
        "        }\n",
        "\n",
        "    def process_multiple_pdfs(self, pdf_directory: str, output_csv: str = \"client_data.csv\", max_pages_per_pdf: int = 20) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Process multiple PDF files and extract client data using GPT-4 Vision\n",
        "\n",
        "        Args:\n",
        "            pdf_directory: Directory containing PDF files\n",
        "            output_csv: Output CSV file name\n",
        "            max_pages_per_pdf: Maximum pages to process per PDF (cost control)\n",
        "\n",
        "        Returns:\n",
        "            List of extracted client data dictionaries\n",
        "        \"\"\"\n",
        "        pdf_files = list(Path(pdf_directory).glob(\"*.pdf\"))\n",
        "        all_client_data = []\n",
        "\n",
        "        logger.info(f\"Found {len(pdf_files)} PDF files to process\")\n",
        "\n",
        "        for pdf_file in pdf_files:\n",
        "            logger.info(f\"Processing: {pdf_file.name}\")\n",
        "\n",
        "            # Use the actual filename as document name\n",
        "            document_name = pdf_file.name\n",
        "\n",
        "            # Convert PDF to images\n",
        "            images = self.pdf_to_images(str(pdf_file), max_pages_per_pdf)\n",
        "\n",
        "            if images:\n",
        "                # Extract client data using GPT-4 Vision\n",
        "                client_data = self.extract_client_data_from_images(images, document_name)\n",
        "                all_client_data.append(client_data)\n",
        "\n",
        "                # Add delay to avoid rate limiting (GPT-4V has stricter limits)\n",
        "                time.sleep(5)\n",
        "            else:\n",
        "                logger.warning(f\"No images extracted from {pdf_file.name}\")\n",
        "                all_client_data.append(self._create_empty_response(document_name, f\"No images extracted from {pdf_file.name}\"))\n",
        "\n",
        "        # Save to CSV\n",
        "        self.save_to_csv(all_client_data, output_csv)\n",
        "\n",
        "        return all_client_data\n",
        "\n",
        "    def save_to_csv(self, client_data_list: List[Dict], output_csv: str):\n",
        "        \"\"\"\n",
        "        Save extracted client data to CSV file - handles multiple entries per document\n",
        "\n",
        "        Args:\n",
        "            client_data_list: List of client data dictionaries\n",
        "            output_csv: Output CSV filename\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Prepare data for CSV\n",
        "            csv_data = []\n",
        "\n",
        "            for data in client_data_list:\n",
        "                # Get document info\n",
        "                doc_info = data.get('document_info', {})\n",
        "                all_entries = data.get('all_entries', [])\n",
        "\n",
        "                # If no entries found, create one row with document info\n",
        "                if not all_entries:\n",
        "                    row = {\n",
        "                        'Document Name': doc_info.get('document_name', ''),\n",
        "                        'Extraction Timestamp': doc_info.get('extraction_timestamp', ''),\n",
        "                        'Pages Processed': doc_info.get('total_pages_processed', ''),\n",
        "                        'Total Entries Found': doc_info.get('total_entries_found', '0'),\n",
        "                        'Entry ID': 'No entries found',\n",
        "                        'Confidence Score': data.get('confidence_score', ''),\n",
        "                        'Notes': data.get('notes', '')\n",
        "                    }\n",
        "\n",
        "                    # Add all the required client data fields with \"Not Found\"\n",
        "                    for field in self.metrics:\n",
        "                        row[field] = 'Not Found'\n",
        "\n",
        "                    csv_data.append(row)\n",
        "                else:\n",
        "                    # Create a row for each entry found\n",
        "                    for entry in all_entries:\n",
        "                        row = {\n",
        "                            'Document Name': doc_info.get('document_name', ''),\n",
        "                            'Extraction Timestamp': doc_info.get('extraction_timestamp', ''),\n",
        "                            'Pages Processed': doc_info.get('total_pages_processed', ''),\n",
        "                            'Total Entries Found': doc_info.get('total_entries_found', str(len(all_entries))),\n",
        "                            'Entry ID': entry.get('entry_id', ''),\n",
        "                            'Confidence Score': data.get('confidence_score', ''),\n",
        "                            'Notes': data.get('notes', '')\n",
        "                        }\n",
        "\n",
        "                        # Add all the required client data fields for this entry\n",
        "                        for field in self.metrics:\n",
        "                            row[field] = entry.get(field, 'Not Found')\n",
        "\n",
        "                        csv_data.append(row)\n",
        "\n",
        "            # Create DataFrame and save to CSV\n",
        "            df = pd.DataFrame(csv_data)\n",
        "            df.to_csv(output_csv, index=False)\n",
        "\n",
        "            logger.info(f\"Client data saved to {output_csv}\")\n",
        "\n",
        "            # Print summary\n",
        "            total_entries = sum(len(data.get('all_entries', [])) for data in client_data_list)\n",
        "            print(f\"\\nProcessing Summary:\")\n",
        "            print(f\"Total documents processed: {len(client_data_list)}\")\n",
        "            print(f\"Total entries extracted: {total_entries}\")\n",
        "            print(f\"Average entries per document: {total_entries/len(client_data_list):.1f}\")\n",
        "            print(f\"Output saved to: {output_csv}\")\n",
        "            print(f\"\\nColumns in output CSV:\")\n",
        "            for col in df.columns:\n",
        "                print(f\"  - {col}\")\n",
        "\n",
        "            # Print extraction statistics\n",
        "            successful_extractions = sum(1 for data in client_data_list if data.get('confidence_score') != 'low')\n",
        "            documents_with_entries = sum(1 for data in client_data_list if len(data.get('all_entries', [])) > 0)\n",
        "\n",
        "            print(f\"\\nExtraction Statistics:\")\n",
        "            print(f\"  - Documents with successful extractions: {successful_extractions}/{len(client_data_list)}\")\n",
        "            print(f\"  - Documents with entries found: {documents_with_entries}/{len(client_data_list)}\")\n",
        "            print(f\"  - Success rate: {(successful_extractions/len(client_data_list)*100):.1f}%\")\n",
        "\n",
        "            # Show entry distribution\n",
        "            entry_counts = [len(data.get('all_entries', [])) for data in client_data_list]\n",
        "            if entry_counts:\n",
        "                print(f\"  - Min entries per document: {min(entry_counts)}\")\n",
        "                print(f\"  - Max entries per document: {max(entry_counts)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving to CSV: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to demonstrate usage\n",
        "    \"\"\"\n",
        "    # Initialize the extractor\n",
        "    API_KEY = userdata.get('OpenApi')\n",
        "    extractor = ClientDataExtractor(API_KEY)\n",
        "\n",
        "    # Process PDFs from a directory\n",
        "    pdf_directory = \"/content/drive/MyDrive/Client_Documents\"\n",
        "    output_file = \"extracted_client_data_update.csv\"\n",
        "\n",
        "    # Create sample directory structure\n",
        "    os.makedirs(pdf_directory, exist_ok=True)\n",
        "\n",
        "    print(f\"Place your client PDF documents in the '{pdf_directory}' directory\")\n",
        "    print(\"The system will extract the following client information:\")\n",
        "    print(\"  - page_number, date, Timestamp, Client_Id\")\n",
        "    print(\"  - Company_Name, Address, Contact/Attorneys\")\n",
        "    print(\"  - Phone, Fax1, Fax2, Send Fx, Status, Email\")\n",
        "    print(\"  - Default_Warrant, Manager, Billing_Type, A/R_Balance\")\n",
        "    print(\"\\n⚠️  Note: GPT-4 Vision is more expensive than text models. Processing is limited to 20 pages per PDF by default.\")\n",
        "\n",
        "    # Check if directory has PDFs\n",
        "    pdf_files = list(Path(pdf_directory).glob(\"*.pdf\"))\n",
        "\n",
        "    if pdf_files:\n",
        "        print(f\"\\nFound {len(pdf_files)} PDF files:\")\n",
        "        for pdf in pdf_files:\n",
        "            print(f\"  - {pdf.name}\")\n",
        "\n",
        "        # Process the PDFs\n",
        "        extracted_data = extractor.process_multiple_pdfs(\n",
        "            pdf_directory,\n",
        "            output_file,\n",
        "            max_pages_per_pdf=20  # Adjust based on your needs and budget\n",
        "        )\n",
        "\n",
        "        print(f\"\\n✅ Processing complete! Check {output_file} for results.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"\\n⚠️  No PDF files found in '{pdf_directory}' directory\")\n",
        "        print(\"Please add your client PDF documents and run again.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOO66z3N_8rf",
        "outputId": "5c30f334-6211-4e65-b70b-4cc1c0ddd50e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Place your client PDF documents in the '/content/drive/MyDrive/Client_Documents' directory\n",
            "The system will extract the following client information:\n",
            "  - page_number, date, Timestamp, Client_Id\n",
            "  - Company_Name, Address, Contact/Attorneys\n",
            "  - Phone, Fax1, Fax2, Send Fx, Status, Email\n",
            "  - Default_Warrant, Manager, Billing_Type, A/R_Balance\n",
            "\n",
            "⚠️  Note: GPT-4 Vision is more expensive than text models. Processing is limited to 20 pages per PDF by default.\n",
            "\n",
            "Found 1 PDF files:\n",
            "  - page 1 client files.pdf\n",
            "\n",
            "=== Converting PDF to Images: /content/drive/MyDrive/Client_Documents/page 1 client files.pdf ===\n",
            "File exists: True\n",
            "File size: 0.08 MB\n",
            "Attempting Method 1: Standard pdf2image...\n",
            "✅ Method 1 Success: Converted 1 pages\n",
            "Processing 1 images for page 1 client files.pdf\n",
            "Encoding image 1/1 - Size: (2550, 3301)\n",
            "Successfully encoded image 1\n",
            "Prepared 1 images for API call\n",
            "Making API call to GPT-4 Vision...\n",
            "Received response from API (length: 6906)\n",
            "Response preview: ```json\n",
            "{\n",
            "    \"document_info\": {\n",
            "        \"document_name\": \"page 1 client files.pdf\",\n",
            "        \"extraction_timestamp\": \"2025-06-24T03:55:20.313582\",\n",
            "        \"total_pages_processed\": \"1\",\n",
            "        \"total_entries_found\": \"9\"\n",
            "    },\n",
            "    \"all_entries\": [\n",
            "        {\n",
            "            \"entry_id\": \"entry_1\",\n",
            "            \"page_number\": \"1\",\n",
            "            \"date\": \"2025-04-21\",\n",
            "            \"Timestamp\": \"6:12\",\n",
            "            \"Client_Id\": \"0002\",\n",
            "            \"Company_Name\": \"LENTNEK MANAGEMENT\",\n",
            "            \"Address\": \"2...\n",
            "Extracted JSON string: {\n",
            "    \"document_info\": {\n",
            "        \"document_name\": \"page 1 client files.pdf\",\n",
            "        \"extraction_timestamp\": \"2025-06-24T03:55:20.313582\",\n",
            "        \"total_pages_processed\": \"1\",\n",
            "        \"total_entries_...\n",
            "\n",
            "Processing Summary:\n",
            "Total documents processed: 1\n",
            "Total entries extracted: 9\n",
            "Average entries per document: 9.0\n",
            "Output saved to: extracted_client_data_update.csv\n",
            "\n",
            "Columns in output CSV:\n",
            "  - Document Name\n",
            "  - Extraction Timestamp\n",
            "  - Pages Processed\n",
            "  - Total Entries Found\n",
            "  - Entry ID\n",
            "  - Confidence Score\n",
            "  - Notes\n",
            "  - page_number\n",
            "  - date\n",
            "  - Timestamp\n",
            "  - Client_Id\n",
            "  - Company_Name\n",
            "  - Address\n",
            "  - Contact/Attorneys\n",
            "  - Phone\n",
            "  - Fax1\n",
            "  - Fax2\n",
            "  - Send Fx1\n",
            "  - Send Fx2\n",
            "  - Status\n",
            "  - Email\n",
            "  - Default_Warrant\n",
            "  - Manager\n",
            "  - Billing_Type\n",
            "  - A/R_Balance\n",
            "\n",
            "Extraction Statistics:\n",
            "  - Documents with successful extractions: 1/1\n",
            "  - Documents with entries found: 1/1\n",
            "  - Success rate: 100.0%\n",
            "  - Min entries per document: 9\n",
            "  - Max entries per document: 9\n",
            "\n",
            "✅ Processing complete! Check extracted_client_data_update.csv for results.\n"
          ]
        }
      ]
    }
  ]
}